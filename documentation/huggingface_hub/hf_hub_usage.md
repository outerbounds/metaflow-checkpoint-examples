<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! Instead, edit the notebook w/the location & name as this file. -->

# `@huggingface_hub`

<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->

- [Usage Patterns](#usage-patterns)
  - [Loading Static Repos For A `@step`](#loading-static-repos-for-a-step)
  - [Loading HF Repos Dynamically](#loading-hf-repos-dynamically)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->


The `@huggingface_hub` decorator simplifies the process of downloading, caching, and managing models from the Hugging Face Hub. It provides seamless integration between Metaflow's datastore and Hugging Face Hub's model repository. This decorator is a syntactic sugar over the `@checkpoint` decorator to easily cache/load models from HuggingFace Hub. All models are stored with same way the `@checkpoint` decorator stores checkpoints i.e. objects are stored under the namespace and step name. The decorator injects a `huggingface_hub` object into the `current` singleton. This object has two main properties: 

1. Exposes a `loaded` property that returns the path to the models loaded via the `@huggingface_hub` decorator's `load` parameter.
2. Provides an wrapper over the [huggingface_hub](https://github.com/huggingface/huggingface_hub)'s [snapshot_download](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/file_download#huggingface_hub.snapshot_download) function. The `current.huggingface_hub.snapshot_download` function returns a reference to the model stored in the datastore. This reference can be used in subsequent steps using the `@model` decorator. 


## Usage Patterns

### Loading Static Repos For A `@step`

Some Flows might require statically hard-coded models/datasets coming from huggingface. The `load` parameter of the `@huggingface_hub` decorator can be used to load these models/datasets. This style of loading models is very useful when models/datasets don't change often and can be hard-coded into the Flow. The models specified in the `load` parameter are downloaded from huggingface hub and stored in the datastore if they are not already present. The path to the model/dataset loaded is accessible via the `loaded` property of the `huggingface_hub` object. The below example shows how to load a static model from huggingface hub and access it in a `@step`. 

The `load` parameter can take multiple form of arguments. Where the simplest for is a list of strings representing the `repo_id`. It can also take a list of dictionaries that provide all the arguments to the [snapshot_download](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/file_download#huggingface_hub.snapshot_download) function. 


```py title="temp_files/hub_deco_flow.py"
from metaflow import FlowSpec, step, huggingface_hub, pypi_base, current
import os

@pypi_base(packages={"huggingface-hub": "0.16.4"})
class SimpleHFFlow(FlowSpec):
    
    @huggingface_hub(load=["bert-base-uncased"])
    @step
    def start(self):
        import os
        # Access the loaded model through current.huggingface_hub.loaded
        model_path = current.huggingface_hub.loaded["bert-base-uncased"]
        print(f"Model loaded at: {model_path}")
        print(f"Contents: {os.listdir(model_path)}")
        self.next(self.load_to_path)
    
    @huggingface_hub(load=[("bert-base-uncased", "./model_directory")])
    @step
    def load_to_path(self):
        import os
        # Access the loaded model through current.huggingface_hub.loaded
        model_path = current.huggingface_hub.loaded["bert-base-uncased"]
        print(f"Model loaded at: {model_path}")
        print(f"Contents: {os.listdir(model_path)}")
        self.next(self.end)
    

    @huggingface_hub(load=[
        {
            "repo_id": "bert-base-uncased",
            "allow_patterns": ["*.json", "tokenizer.txt"],
            "repo_type": "model"
        },
    ])
    @step
    def end(self):
        # Access the loaded model through current.huggingface_hub.loaded
        model_path = current.huggingface_hub.loaded["bert-base-uncased"]
        print(f"Model loaded at: {model_path}")
        print(f"Contents: {os.listdir(model_path)}")
        

if __name__ == "__main__":
    SimpleHFFlow()
```

<CodeOutputBlock lang="bash">

```
    ...

    ...

    ...

    ...

    ...

    ...
     [7472/start/47522 (pid 2223466)] Task is starting.
    ...

    ...
     [7472/start/47522 (pid 2223466)] [@huggingface_hub] Loading model from datastore to /tmp/metaflow_hf_9b5c6e8800_kbi7fnx5. Model being loaded: mf.huggingface_hub/checkpoints/artifacts/SimpleHFFlow/start/26ec4b03ee0e/a92ae9615600/8bcbe12d.0.9b5c6e8800.0
    ...

    ...
     [7472/start/47522 (pid 2223466)] Model loaded at: /tmp/metaflow_hf_9b5c6e8800_kbi7fnx5
     [7472/start/47522 (pid 2223466)] Contents: ['README.md', 'config.json', '.gitattributes', 'LICENSE', 'flax_model.msgpack', 'tf_model.h5', 'tokenizer.json', 'pytorch_model.bin', 'vocab.txt', 'model.onnx', 'rust_model.ot', 'model.safetensors', 'tokenizer_config.json']
    ...

    ...
     [7472/start/47522 (pid 2223466)] Task finished successfully.
    ...

    ...
     [7472/load_to_path/47523 (pid 2223643)] Task is starting.
    ...

    ...
     [7472/load_to_path/47523 (pid 2223643)] [@huggingface_hub] Loading model from datastore to ./model_directory. Model being loaded: mf.huggingface_hub/checkpoints/artifacts/SimpleHFFlow/load_to_path/26ec4b03ee0e/1a69a1ab0540/4b90315a.0.9b5c6e8800.0
    ...

    ...
     [7472/load_to_path/47523 (pid 2223643)] Model loaded at: ./model_directory
     [7472/load_to_path/47523 (pid 2223643)] Contents: ['README.md', 'config.json', '.gitattributes', 'LICENSE', 'flax_model.msgpack', 'tf_model.h5', 'tokenizer.json', 'pytorch_model.bin', 'coreml', 'vocab.txt', 'model.onnx', 'rust_model.ot', 'model.safetensors', 'tokenizer_config.json']
    ...

    ...
     [7472/load_to_path/47523 (pid 2223643)] Task finished successfully.
    ...

    ...
     [7472/end/47524 (pid 2223804)] Task is starting.
    ...

    ...
     [7472/end/47524 (pid 2223804)] [@huggingface_hub] Loading model from datastore to /tmp/metaflow_hf_9b5c6e8800_yfsk2bdb. Model being loaded: mf.huggingface_hub/checkpoints/artifacts/SimpleHFFlow/end/26ec4b03ee0e/24105546e482/73eef1dc.0.9b5c6e8800.0
    ...

    ...
     [7472/end/47524 (pid 2223804)] Model loaded at: /tmp/metaflow_hf_9b5c6e8800_yfsk2bdb
     [7472/end/47524 (pid 2223804)] Contents: ['config.json', 'tokenizer.json', 'tokenizer_config.json']
    ...

    ...
     [7472/end/47524 (pid 2223804)] Task finished successfully.
    ...

    ...
```

</CodeOutputBlock>

### Loading HF Repos Dynamically

In many cases, Huggingface models or datasets might be passed down as parameters to the Flow. This approach can make it challenging to load them using the `load` parameter of the `@huggingface_hub` decorator. For this case the `current.huggingface_hub` provides a `snapshot_download` function that can be used to download the model/dataset from huggingface hub and return a reference to the model/dataset. This reference can be used in subsequent steps using the `@model` decorator. The core difference between the `load` parameter and the `snapshot_download` function is that the `load` parameter is used to load static models/datasets while the `snapshot_download` function will return a reference that can be loaded in future steps. If the `force_download` parameter is passed to the `snapshot_download` function, it will bust the cache, download the model/dataset again and store it in the datastore. 

The below example shows how to load a dynamic model from huggingface hub and access it in a `@step`. 


```py title="temp_files/hub_deco_flow_2.py"
from metaflow import FlowSpec, step, current, huggingface_hub, model
import os

class SimpleHFFlow(FlowSpec):
    
    @huggingface_hub
    @step
    def start(self):
        # Download a small model from HuggingFace Hub
        self.hf_model_reference = current.huggingface_hub.snapshot_download(
            repo_id="bert-base-uncased",
            allow_patterns=["*.json"]  # Only download the config file to keep it light
        )
        print(f"Model Reference saved with key : %s" % self.hf_model_reference["key"])
        self.next(self.end)
    
    @model(load="hf_model_reference")
    @step
    def end(self):
        print(f"Model loaded at: {current.model.loaded['hf_model_reference']}")
        print(f"Contents: {os.listdir(current.model.loaded['hf_model_reference'])}")

if __name__ == "__main__":
    SimpleHFFlow()
```

<CodeOutputBlock lang="bash">

```
    ...

    ...

    ...

    ...

    ...
     [7473/start/47526 (pid 2223882)] Task is starting.
    ...

    ...
     [7473/start/47526 (pid 2223882)] Model Reference saved with key : mf.huggingface_hub/checkpoints/artifacts/SimpleHFFlow/start/26ec4b03ee0e/a92ae9615600/8bcbe12d.0.9b5c6e8800.0
    ...

    ...
     [7473/start/47526 (pid 2223882)] Task finished successfully.
    ...

    ...
     [7473/end/47527 (pid 2223935)] Task is starting.
    ...

    ...
     [7473/end/47527 (pid 2223935)] [@model] Loading Artifact with name `hf_model_reference` [type:checkpoint] with key: mf.huggingface_hub/checkpoints/artifacts/SimpleHFFlow/start/26ec4b03ee0e/a92ae9615600/8bcbe12d.0.9b5c6e8800.0
    ...

    ...
     [7473/end/47527 (pid 2223935)] [@model] Loaded artifact `hf_model_reference[type:checkpoint]` in 25.73 seconds
    ...

    ...
     [7473/end/47527 (pid 2223935)] Model loaded at: /tmp/metaflow_models_hf_model_reference_4yejtsvy
     [7473/end/47527 (pid 2223935)] Contents: ['README.md', 'config.json', '.gitattributes', 'LICENSE', 'flax_model.msgpack', 'tf_model.h5', 'tokenizer.json', 'pytorch_model.bin', 'vocab.txt', 'model.onnx', 'rust_model.ot', 'model.safetensors', 'tokenizer_config.json']
    ...

    ...
     [7473/end/47527 (pid 2223935)] Task finished successfully.
    ...

    ...
```

</CodeOutputBlock>
