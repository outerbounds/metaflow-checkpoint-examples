{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T15:13:32.835523Z",
     "iopub.status.busy": "2024-12-11T15:13:32.835072Z",
     "iopub.status.idle": "2024-12-11T15:13:32.841878Z",
     "shell.execute_reply": "2024-12-11T15:13:32.841438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: METAFLOW_PROFILE=dev-valay\n",
      "env: METAFLOW_UI_URL=\n"
     ]
    }
   ],
   "source": [
    "#meta:tag=hide\n",
    "%env METAFLOW_PROFILE=dev-valay\n",
    "%env METAFLOW_UI_URL=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T15:13:32.875844Z",
     "iopub.status.busy": "2024-12-11T15:13:32.875429Z",
     "iopub.status.idle": "2024-12-11T15:13:32.878300Z",
     "shell.execute_reply": "2024-12-11T15:13:32.877861Z"
    }
   },
   "outputs": [],
   "source": [
    "#meta:tag=hide\n",
    "import os\n",
    "os.makedirs(\"temp_files\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `@huggingface_hub`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- START doctoc -->\n",
    "<!-- END doctoc -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@huggingface_hub` decorator simplifies the process of downloading, caching, and managing models from the Hugging Face Hub. It provides seamless integration between Metaflow's datastore and Hugging Face Hub's model repository. This decorator is a syntactic sugar over the `@checkpoint` decorator to easily cache/load models from HuggingFace Hub. All models are stored with same way the `@checkpoint` decorator stores checkpoints i.e. objects are stored under the namespace and step name. The decorator injects a `huggingface_hub` object into the `current` singleton. This object has two main properties: \n",
    "\n",
    "1. Exposes a `loaded` property that returns the path to the models loaded via the `@huggingface_hub` decorator's `load` parameter.\n",
    "2. Provides an wrapper over the [huggingface_hub](https://github.com/huggingface/huggingface_hub)'s [snapshot_download](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/file_download#huggingface_hub.snapshot_download) function. The `current.huggingface_hub.snapshot_download` function returns a reference to the model stored in the datastore. This reference can be used in subsequent steps using the `@model` decorator. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Patterns\n",
    "\n",
    "### Loading Static Repos For A `@step`\n",
    "\n",
    "Some Flows might require statically hard-coded models/datasets coming from huggingface. The `load` parameter of the `@huggingface_hub` decorator can be used to load these models/datasets. This style of loading models is very useful when models/datasets don't change often and can be hard-coded into the Flow. The models specified in the `load` parameter are downloaded from huggingface hub and stored in the datastore if they are not already present. The path to the model/dataset loaded is accessible via the `loaded` property of the `huggingface_hub` object. The below example shows how to load a static model from huggingface hub and access it in a `@step`. \n",
    "\n",
    "The `load` parameter can take multiple form of arguments. Where the simplest for is a list of strings representing the `repo_id`. It can also take a list of dictionaries that provide all the arguments to the [snapshot_download](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/file_download#huggingface_hub.snapshot_download) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T15:13:32.880494Z",
     "iopub.status.busy": "2024-12-11T15:13:32.880115Z",
     "iopub.status.idle": "2024-12-11T15:13:32.884214Z",
     "shell.execute_reply": "2024-12-11T15:13:32.883725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting temp_files/hub_deco_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile temp_files/hub_deco_flow.py\n",
    "#meta:tag=hide_output\n",
    "from metaflow import FlowSpec, step, huggingface_hub, pypi_base, current\n",
    "import os\n",
    "\n",
    "@pypi_base(packages={\"huggingface-hub\": \"0.16.4\"})\n",
    "class SimpleHFFlow(FlowSpec):\n",
    "    \n",
    "    @huggingface_hub(load=[\"bert-base-uncased\"])\n",
    "    @step\n",
    "    def start(self):\n",
    "        import os\n",
    "        # Access the loaded model through current.huggingface_hub.loaded\n",
    "        model_path = current.huggingface_hub.loaded[\"bert-base-uncased\"]\n",
    "        print(f\"Model loaded at: {model_path}\")\n",
    "        print(f\"Contents: {os.listdir(model_path)}\")\n",
    "        self.next(self.load_to_path)\n",
    "    \n",
    "    @huggingface_hub(load=[(\"bert-base-uncased\", \"./model_directory\")])\n",
    "    @step\n",
    "    def load_to_path(self):\n",
    "        import os\n",
    "        # Access the loaded model through current.huggingface_hub.loaded\n",
    "        model_path = current.huggingface_hub.loaded[\"bert-base-uncased\"]\n",
    "        print(f\"Model loaded at: {model_path}\")\n",
    "        print(f\"Contents: {os.listdir(model_path)}\")\n",
    "        self.next(self.end)\n",
    "    \n",
    "\n",
    "    @huggingface_hub(load=[\n",
    "        {\n",
    "            \"repo_id\": \"bert-base-uncased\",\n",
    "            \"allow_patterns\": [\"*.json\", \"tokenizer.txt\"],\n",
    "            \"repo_type\": \"model\"\n",
    "        },\n",
    "    ])\n",
    "    @step\n",
    "    def end(self):\n",
    "        # Access the loaded model through current.huggingface_hub.loaded\n",
    "        model_path = current.huggingface_hub.loaded[\"bert-base-uncased\"]\n",
    "        print(f\"Model loaded at: {model_path}\")\n",
    "        print(f\"Contents: {os.listdir(model_path)}\")\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SimpleHFFlow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T15:13:32.886429Z",
     "iopub.status.busy": "2024-12-11T15:13:32.885994Z",
     "iopub.status.idle": "2024-12-11T15:14:40.178369Z",
     "shell.execute_reply": "2024-12-11T15:14:40.177729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.12.36.post9-git09d02cb-dirty+obcheckpoint(0.1.4);ob(v1)\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mSimpleHFFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:valay@outerbounds.co\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\r\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\r\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\r\n",
      "\u001b[35m2024-12-11 07:13:34.971 \u001b[0m\u001b[22mBootstrapping virtual environment(s) ...\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:13:35.093 \u001b[0m\u001b[22mVirtual environment(s) bootstrapped!\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:13:37.881 \u001b[0m\u001b[1mWorkflow starting (run-id 7472):\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:13:38.765 \u001b[0m\u001b[32m[7472/start/47522 (pid 2223466)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:13:40.470 \u001b[0m\u001b[32m[7472/start/47522 (pid 2223466)] \u001b[0m\u001b[22m[@huggingface_hub] Loading model from datastore to /tmp/metaflow_hf_9b5c6e8800_kbi7fnx5. Model being loaded: mf.huggingface_hub/checkpoints/artifacts/SimpleHFFlow/start/26ec4b03ee0e/a92ae9615600/8bcbe12d.0.9b5c6e8800.0\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:14:05.718 \u001b[0m\u001b[32m[7472/start/47522 (pid 2223466)] \u001b[0m\u001b[22mModel loaded at: /tmp/metaflow_hf_9b5c6e8800_kbi7fnx5\u001b[0m\r\n",
      "\u001b[35m2024-12-11 07:14:05.719 \u001b[0m\u001b[32m[7472/start/47522 (pid 2223466)] \u001b[0m\u001b[22mContents: ['README.md', 'config.json', '.gitattributes', 'LICENSE', 'flax_model.msgpack', 'tf_model.h5', 'tokenizer.json', 'pytorch_model.bin', 'vocab.txt', 'model.onnx', 'rust_model.ot', 'model.safetensors', 'tokenizer_config.json']\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:14:06.810 \u001b[0m\u001b[32m[7472/start/47522 (pid 2223466)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:14:07.215 \u001b[0m\u001b[32m[7472/load_to_path/47523 (pid 2223643)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:14:09.521 \u001b[0m\u001b[32m[7472/load_to_path/47523 (pid 2223643)] \u001b[0m\u001b[22m[@huggingface_hub] Loading model from datastore to ./model_directory. Model being loaded: mf.huggingface_hub/checkpoints/artifacts/SimpleHFFlow/load_to_path/26ec4b03ee0e/1a69a1ab0540/4b90315a.0.9b5c6e8800.0\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:14:34.421 \u001b[0m\u001b[32m[7472/load_to_path/47523 (pid 2223643)] \u001b[0m\u001b[22mModel loaded at: ./model_directory\u001b[0m\r\n",
      "\u001b[35m2024-12-11 07:14:34.421 \u001b[0m\u001b[32m[7472/load_to_path/47523 (pid 2223643)] \u001b[0m\u001b[22mContents: ['README.md', 'config.json', '.gitattributes', 'LICENSE', 'flax_model.msgpack', 'tf_model.h5', 'tokenizer.json', 'pytorch_model.bin', 'coreml', 'vocab.txt', 'model.onnx', 'rust_model.ot', 'model.safetensors', 'tokenizer_config.json']\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:14:35.200 \u001b[0m\u001b[32m[7472/load_to_path/47523 (pid 2223643)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:14:35.577 \u001b[0m\u001b[32m[7472/end/47524 (pid 2223804)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:14:37.338 \u001b[0m\u001b[32m[7472/end/47524 (pid 2223804)] \u001b[0m\u001b[22m[@huggingface_hub] Loading model from datastore to /tmp/metaflow_hf_9b5c6e8800_yfsk2bdb. Model being loaded: mf.huggingface_hub/checkpoints/artifacts/SimpleHFFlow/end/26ec4b03ee0e/24105546e482/73eef1dc.0.9b5c6e8800.0\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:14:38.566 \u001b[0m\u001b[32m[7472/end/47524 (pid 2223804)] \u001b[0m\u001b[22mModel loaded at: /tmp/metaflow_hf_9b5c6e8800_yfsk2bdb\u001b[0m\r\n",
      "\u001b[35m2024-12-11 07:14:38.566 \u001b[0m\u001b[32m[7472/end/47524 (pid 2223804)] \u001b[0m\u001b[22mContents: ['config.json', 'tokenizer.json', 'tokenizer_config.json']\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:14:39.334 \u001b[0m\u001b[32m[7472/end/47524 (pid 2223804)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:14:39.457 \u001b[0m\u001b[1mDone!\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "#meta:tag=hide_input\n",
    "#meta:show_steps=start,load_to_path,end\n",
    "! python temp_files/hub_deco_flow.py --environment=pypi run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading HF Repos Dynamically\n",
    "\n",
    "In many cases, Huggingface models or datasets might be passed down as parameters to the Flow. This approach can make it challenging to load them using the `load` parameter of the `@huggingface_hub` decorator. For this case the `current.huggingface_hub` provides a `snapshot_download` function that can be used to download the model/dataset from huggingface hub and return a reference to the model/dataset. This reference can be used in subsequent steps using the `@model` decorator. The core difference between the `load` parameter and the `snapshot_download` function is that the `load` parameter is used to load static models/datasets while the `snapshot_download` function will return a reference that can be loaded in future steps. If the `force_download` parameter is passed to the `snapshot_download` function, it will bust the cache, download the model/dataset again and store it in the datastore. \n",
    "\n",
    "The below example shows how to load a dynamic model from huggingface hub and access it in a `@step`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T15:14:40.182434Z",
     "iopub.status.busy": "2024-12-11T15:14:40.181996Z",
     "iopub.status.idle": "2024-12-11T15:14:40.186531Z",
     "shell.execute_reply": "2024-12-11T15:14:40.186116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting temp_files/hub_deco_flow_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile temp_files/hub_deco_flow_2.py\n",
    "#meta:tag=hide_output\n",
    "from metaflow import FlowSpec, step, current, huggingface_hub, model\n",
    "import os\n",
    "\n",
    "class SimpleHFFlow(FlowSpec):\n",
    "    \n",
    "    @huggingface_hub\n",
    "    @step\n",
    "    def start(self):\n",
    "        # Download a small model from HuggingFace Hub\n",
    "        self.hf_model_reference = current.huggingface_hub.snapshot_download(\n",
    "            repo_id=\"bert-base-uncased\",\n",
    "            allow_patterns=[\"*.json\"]  # Only download the config file to keep it light\n",
    "        )\n",
    "        print(f\"Model Reference saved with key : %s\" % self.hf_model_reference[\"key\"])\n",
    "        self.next(self.end)\n",
    "    \n",
    "    @model(load=\"hf_model_reference\")\n",
    "    @step\n",
    "    def end(self):\n",
    "        print(f\"Model loaded at: {current.model.loaded['hf_model_reference']}\")\n",
    "        print(f\"Contents: {os.listdir(current.model.loaded['hf_model_reference'])}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SimpleHFFlow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T15:14:40.188628Z",
     "iopub.status.busy": "2024-12-11T15:14:40.188298Z",
     "iopub.status.idle": "2024-12-11T15:15:21.758114Z",
     "shell.execute_reply": "2024-12-11T15:15:21.757468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.12.36.post9-git09d02cb-dirty+obcheckpoint(0.1.4);ob(v1)\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mSimpleHFFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:valay@outerbounds.co\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\r\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\r\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:14:45.090 \u001b[0m\u001b[1mWorkflow starting (run-id 7473):\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:14:46.220 \u001b[0m\u001b[32m[7473/start/47526 (pid 2223882)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:14:48.438 \u001b[0m\u001b[32m[7473/start/47526 (pid 2223882)] \u001b[0m\u001b[22mModel Reference saved with key : mf.huggingface_hub/checkpoints/artifacts/SimpleHFFlow/start/26ec4b03ee0e/a92ae9615600/8bcbe12d.0.9b5c6e8800.0\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:14:49.258 \u001b[0m\u001b[32m[7473/start/47526 (pid 2223882)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:14:49.563 \u001b[0m\u001b[32m[7473/end/47527 (pid 2223935)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:14:50.743 \u001b[0m\u001b[32m[7473/end/47527 (pid 2223935)] \u001b[0m\u001b[22m[@model] Loading Artifact with name `hf_model_reference` [type:checkpoint] with key: mf.huggingface_hub/checkpoints/artifacts/SimpleHFFlow/start/26ec4b03ee0e/a92ae9615600/8bcbe12d.0.9b5c6e8800.0\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:15:16.468 \u001b[0m\u001b[32m[7473/end/47527 (pid 2223935)] \u001b[0m\u001b[22m[@model] Loaded artifact `hf_model_reference[type:checkpoint]` in 25.73 seconds\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:15:16.491 \u001b[0m\u001b[32m[7473/end/47527 (pid 2223935)] \u001b[0m\u001b[22mModel loaded at: /tmp/metaflow_models_hf_model_reference_4yejtsvy\u001b[0m\r\n",
      "\u001b[35m2024-12-11 07:15:16.491 \u001b[0m\u001b[32m[7473/end/47527 (pid 2223935)] \u001b[0m\u001b[22mContents: ['README.md', 'config.json', '.gitattributes', 'LICENSE', 'flax_model.msgpack', 'tf_model.h5', 'tokenizer.json', 'pytorch_model.bin', 'vocab.txt', 'model.onnx', 'rust_model.ot', 'model.safetensors', 'tokenizer_config.json']\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:15:20.901 \u001b[0m\u001b[32m[7473/end/47527 (pid 2223935)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2024-12-11 07:15:21.016 \u001b[0m\u001b[1mDone!\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "#meta:tag=hide_input\n",
    "#meta:show_steps=start,end\n",
    "! python temp_files/hub_deco_flow_2.py run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mf-modeling-utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
