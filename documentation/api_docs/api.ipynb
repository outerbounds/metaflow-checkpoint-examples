{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T22:56:08.821204Z",
     "iopub.status.busy": "2025-09-17T22:56:08.820849Z",
     "iopub.status.idle": "2025-09-17T22:56:08.827781Z",
     "shell.execute_reply": "2025-09-17T22:56:08.827133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: METAFLOW_PROFILE=dev-valay\n",
      "env: METAFLOW_UI_URL=\n"
     ]
    }
   ],
   "source": [
    "#meta:tag=hide\n",
    "%env METAFLOW_PROFILE=dev-valay\n",
    "%env METAFLOW_UI_URL=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T22:56:08.860905Z",
     "iopub.status.busy": "2025-09-17T22:56:08.860386Z",
     "iopub.status.idle": "2025-09-17T22:56:10.809474Z",
     "shell.execute_reply": "2025-09-17T22:56:10.808847Z"
    }
   },
   "outputs": [],
   "source": [
    "#meta:tag=hide\n",
    "from functools import partial\n",
    "from nbdoc.showdoc import ShowDoc as SD\n",
    "ShowDoc = partial(SD, module_nm='metaflow')\n",
    "from metaflow import huggingface_hub , Checkpoint, checkpoint, model, load_model\n",
    "from metaflow_extensions.obcheckpoint.plugins.machine_learning_utilities.hf_hub.decorator import HuggingfaceRegistry, HuggingfaceLoadedModels\n",
    "from metaflow_extensions.obcheckpoint.plugins.machine_learning_utilities.modeling_utils.core import ModelSerializer, LoadedModels\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T22:56:10.812881Z",
     "iopub.status.busy": "2025-09-17T22:56:10.812387Z",
     "iopub.status.idle": "2025-09-17T22:56:10.832017Z",
     "shell.execute_reply": "2025-09-17T22:56:10.831336Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/ob-packages-fresh-3/lib/python3.10/site-packages/numpydoc/docscrape.py:434: UserWarning: Unknown section Mf Add To Current in the docstring of HuggingfaceHubDecorator in /home/ubuntu/metaflow-checkpoint/metaflow_extensions/obcheckpoint/plugins/machine_learning_utilities/hf_hub/decorator.py.\n",
      "  warn(msg)\n",
      "/home/ubuntu/miniconda3/envs/ob-packages-fresh-3/lib/python3.10/site-packages/numpydoc/docscrape.py:434: UserWarning: Unknown section @@ Returns in the docstring of HuggingfaceHubDecorator in /home/ubuntu/metaflow-checkpoint/metaflow_extensions/obcheckpoint/plugins/machine_learning_utilities/hf_hub/decorator.py.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "# Decorator: `@huggingface_hub`\n",
       "\n",
       "```python\n",
       "@huggingface_hub(...)\n",
       "```\n",
       "\n",
       "**Module:** `metaflow`\n",
       "\n",
       "Decorator that helps cache, version, and store models/datasets from the Hugging Face Hub.\n",
       "\n",
       "## Parameters\n",
       "\n",
       "- **temp_dir_root** (*str, optional*): The root directory that will hold the temporary directory where objects will be downloaded.\n",
       "- **cache_scope** (*str, optional*): The scope of the cache. Can be `checkpoint` / `flow` / `global`.\n",
       "    - `checkpoint` (default): All repos are stored like objects saved by `@checkpoint`.\n",
       "        i.e., the cached path is derived from the namespace, flow, step, and Metaflow foreach iteration.\n",
       "        Any repo downloaded under this scope will only be retrieved from the cache when the step runs under the same namespace in the same flow (at the same foreach index).\n",
       "\n",
       "    - `flow`: All repos are cached under the flow, regardless of namespace.\n",
       "        i.e., the cached path is derived solely from the flow name.\n",
       "        When to use this mode:\n",
       "            - Multiple users are executing the same flow and want shared access to the repos cached by the decorator.\n",
       "            - Multiple versions of a flow are deployed, all needing access to the same repos cached by the decorator.\n",
       "\n",
       "    - `global`: All repos are cached under a globally static path.\n",
       "        i.e., the base path of the cache is static and all repos are stored under it.\n",
       "        When to use this mode:\n",
       "            - All repos from the Hugging Face Hub need to be shared by users across all flow executions.\n",
       "\n",
       "Each caching scope comes with its own trade-offs:\n",
       "    - `checkpoint`:\n",
       "        - Has explicit control over when caches are populated (controlled by the same flow that has the `@huggingface_hub` decorator) but ends up hitting the Hugging Face Hub more often if there are many users/namespaces/steps.\n",
       "        - Since objects are written on a `namespace/flow/step` basis, the blast radius of a bad checkpoint is limited to a particular flow in a namespace.\n",
       "    - `flow`:\n",
       "        - Has less control over when caches are populated (can be written by any execution instance of a flow from any namespace) but results in more cache hits.\n",
       "        - The blast radius of a bad checkpoint is limited to all runs of a particular flow.\n",
       "        - It doesn't promote cache reuse across flows.\n",
       "    - `global`:\n",
       "        - Has no control over when caches are populated (can be written by any flow execution) but has the highest cache hit rate.\n",
       "        - It promotes cache reuse across flows.\n",
       "        - The blast radius of a bad checkpoint spans every flow that could be using a particular repo.\n",
       "- **load: Union[List[str], List[Tuple[Dict, str]], List[Tuple[str, str]], List[Dict], None]**: The list of repos (models/datasets) to load.\n",
       "\n",
       "Loaded repos can be accessed via `current.huggingface_hub.loaded`. If load is set, then the following happens:\n",
       "\n",
       "- If repo (model/dataset) is not found in the datastore:\n",
       "    - Downloads the repo from Hugging Face Hub to a temporary directory (or uses specified path) for local access\n",
       "    - Stores it in Metaflow's datastore (s3/gcs/azure etc.) with a unique name based on repo_type/repo_id\n",
       "        - All HF models loaded for a `@step` will be cached separately under flow/step/namespace.\n",
       "\n",
       "- If repo is found in the datastore:\n",
       "    - Loads it directly from datastore to local path (can be temporary directory or specified path)\n",
       "\n",
       "## Examples\n",
       "\n",
       "```python\n",
       "# **Usage: creating references to models from the Hugging Face Hub that may be loaded in downstream steps**\n",
       "@huggingface_hub\n",
       "@step\n",
       "def pull_model_from_huggingface(self):\n",
       "    # `current.huggingface_hub.snapshot_download` downloads the model from the Hugging Face Hub\n",
       "    # and saves it in the backend storage based on the model's `repo_id`. If there exists a model\n",
       "    # with the same `repo_id` in the backend storage, it will not download the model again. The return\n",
       "    # value of the function is a reference to the model in the backend storage.\n",
       "    # This reference can be used to load the model in the subsequent steps via `@model(load=[\"llama_model\"])`\n",
       "\n",
       "    self.model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
       "    self.llama_model = current.huggingface_hub.snapshot_download(\n",
       "        repo_id=self.model_id,\n",
       "        allow_patterns=[\"*.safetensors\", \"*.json\", \"tokenizer.*\"],\n",
       "    )\n",
       "    self.next(self.train)\n",
       "\n",
       "# **Usage: explicitly loading models at runtime from the Hugging Face Hub or from cache (from Metaflow's datastore)**\n",
       "@huggingface_hub\n",
       "@step\n",
       "def run_training(self):\n",
       "    # Temporary directory (auto-cleaned on exit)\n",
       "    with current.huggingface_hub.load(\n",
       "        repo_id=\"google-bert/bert-base-uncased\",\n",
       "        allow_patterns=[\"*.bin\"],\n",
       "    ) as local_path:\n",
       "        # Use files under local_path\n",
       "        train_model(local_path)\n",
       "        ...\n",
       "\n",
       "# **Usage: loading models directly from the Hugging Face Hub or from cache (from Metaflow's datastore)**\n",
       "\n",
       "@huggingface_hub(load=[\"mistralai/Mistral-7B-Instruct-v0.1\"])\n",
       "@step\n",
       "def pull_model_from_huggingface(self):\n",
       "    path_to_model = current.huggingface_hub.loaded[\"mistralai/Mistral-7B-Instruct-v0.1\"]\n",
       "\n",
       "@huggingface_hub(load=[(\"mistralai/Mistral-7B-Instruct-v0.1\", \"/my-directory\"), (\"myorg/mistral-lora\", \"/my-lora-directory\")])\n",
       "@step\n",
       "def finetune_model(self):\n",
       "    path_to_model = current.huggingface_hub.loaded[\"mistralai/Mistral-7B-Instruct-v0.1\"]\n",
       "    # path_to_model will be /my-directory\n",
       "\n",
       "# Takes all the arguments passed to `snapshot_download`\n",
       "# except for `local_dir`\n",
       "@huggingface_hub(load=[\n",
       "    {\n",
       "        \"repo_id\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
       "    },\n",
       "    {\n",
       "        \"repo_id\": \"myorg/mistral-lora\",\n",
       "        \"repo_type\": \"model\",\n",
       "    },\n",
       "])\n",
       "@step\n",
       "def finetune_model(self):\n",
       "    path_to_model = current.huggingface_hub.loaded[\"mistralai/Mistral-7B-Instruct-v0.1\"]\n",
       "    # path_to_model will be /my-directory\n",
       "```"
      ],
      "text/plain": [
       "<nbdoc.showdoc.ShowDoc at 0x7f616c43f100>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowDoc(huggingface_hub, show_import=True, decorator=True, skip_sections=[\"Mf Add To Current\"], markdown=True, hd_lvl=1)\n",
    "# You can use one of the following sections: \n",
    "# https://github.com/numpy/numpydoc/blob/865b865b3d1dfb8cd2139fa76cefe3643e1bf52a/numpydoc/docscrape.py#L118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T22:56:10.834792Z",
     "iopub.status.busy": "2025-09-17T22:56:10.834416Z",
     "iopub.status.idle": "2025-09-17T22:56:10.845746Z",
     "shell.execute_reply": "2025-09-17T22:56:10.845179Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "## Property: `current.huggingface_hub`\n",
       "\n",
       "This object provides a thin, Metaflow-friendly layer over huggingface_hub's [snapshot_download](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/file_download#huggingface_hub.snapshot_download):\n",
       "\n",
       "- Snapshot references (persist-and-reuse): Use `current.huggingface_hub.snapshot_download(repo_id=..., ...)` to ensure a repo is available in the Metaflow datastore. If absent, it is downloaded once and saved; the call returns a reference dict you can store and load later (for example via `@model`).\n",
       "\n",
       "\n",
       "\n",
       "- On-demand local access (context manager):  Use `current.huggingface_hub.load(repo_id=..., [path=...], ...)` as a context manager to obtain a local filesystem path for immediate use. If the repo exists in the datastore, it is loaded from there; otherwise it is fetched from the Hugging Face Hub and then cached in the datastore. When `path` is omitted, a temporary directory is created and cleaned up automatically when the context exits. When `path` is provided, files are placed there and are not cleaned up by the context manager.\n",
       "\n",
       "\n",
       "\n",
       "Repos are cached in the datastore using the huggingface_hub.snapshot_download's arguments. The cache\n",
       "\n",
       "key may include: `repo_id`, `repo_type`, `revision`, `ignore_patterns`,\n",
       "\n",
       "and `allow_patterns` (see `cache_scope` for how keys are scoped).\n",
       "\n",
       "### Attributes\n",
       "\n",
       "- **loaded**: This property provides a dictionary-like interface to access the local paths of the huggingface repos specified in the `load` argument of the `@huggingface_hub` decorator.\n",
       "\n",
       "### Examples\n",
       "\n",
       "```python\n",
       "# Snapshot reference:\n",
       "ref = current.huggingface_hub.snapshot_download(\n",
       "    repo_id=\"google-bert/bert-base-uncased\",\n",
       "    allow_patterns=[\"*.json\"]\n",
       ")\n",
       "# Explicit Model Loading with Context manager:\n",
       "\n",
       "with current.huggingface_hub.load(\n",
       "    repo_id=\"google-bert/bert-base-uncased\",\n",
       "    allow_patterns=[\"*.json\"]\n",
       ") as local_path:\n",
       "    my_model = torch.load(os.path.join(local_path, \"model.bin\"))\n",
       "```"
      ],
      "text/plain": [
       "<nbdoc.showdoc.ShowDoc at 0x7f6154238fa0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowDoc(HuggingfaceRegistry, spoofstr=\"\", show_import=False, skip_sections=[\"Mf Add To Current\"], name=\"current.huggingface_hub\", markdown=True, hd_lvl=2, type_override=\"property\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T22:56:10.848116Z",
     "iopub.status.busy": "2025-09-17T22:56:10.847647Z",
     "iopub.status.idle": "2025-09-17T22:56:10.852523Z",
     "shell.execute_reply": "2025-09-17T22:56:10.851998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "### Method: `current.huggingface_hub.snapshot_download`\n",
       "\n",
       "Downloads a model from the Hugging Face Hub and caches it in the Metaflow datastore.\n",
       "\n",
       "It passes all parameters to the `huggingface_hub.snapshot_download` function.\n",
       "\n",
       "#### Returns\n",
       "\n",
       "- **** (*dict*): A reference to the artifact saved to or retrieved from the Metaflow datastore."
      ],
      "text/plain": [
       "<nbdoc.showdoc.ShowDoc at 0x7f61541dbf70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowDoc(HuggingfaceRegistry.snapshot_download, spoofstr=\"\", show_import=False, skip_sections=[\"Mf Add To Current\"], name=\"current.huggingface_hub.snapshot_download\", hd_lvl=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T22:56:10.854921Z",
     "iopub.status.busy": "2025-09-17T22:56:10.854530Z",
     "iopub.status.idle": "2025-09-17T22:56:10.860411Z",
     "shell.execute_reply": "2025-09-17T22:56:10.859682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "### Method: `current.huggingface_hub.load`\n",
       "\n",
       "Context manager to load a Hugging Face repo (model/dataset) to a local path.\n",
       "\n",
       "- If `path` is provided, the repo is loaded there and the same path is yielded.\n",
       "\n",
       "- If `path` is not provided, a temporary directory is created, the repo is\n",
       "\n",
       "  loaded there, the path is yielded, and the directory is cleaned up when\n",
       "\n",
       "  the context exits.\n",
       "\n",
       "#### Parameters\n",
       "\n",
       "- **repo_id** (*str, optional*): The Hugging Face repo ID. If omitted, must be provided via kwargs[\"repo_id\"].\n",
       "- **path** (*str, optional*): Target directory to place files. If None, a temp directory is created.\n",
       "- **repo_type** (*str, optional*): Repo type (e.g., \"model\", \"dataset\"). Defaults to \"model\".\n",
       "- ****kwargs** (*Any*): Additional args forwarded to snapshot_download (e.g. force_download, revision,\n",
       "allow_patterns, ignore_patterns, etc.).\n",
       "\n",
       "#### Yields\n",
       "\n",
       "- **** (*str*): Local filesystem path where the repo is available."
      ],
      "text/plain": [
       "<nbdoc.showdoc.ShowDoc at 0x7f616c475c30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowDoc(HuggingfaceRegistry.load, spoofstr=\"\", show_import=False, skip_sections=[\"Mf Add To Current\"], name=\"current.huggingface_hub.load\", hd_lvl=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T22:56:10.862620Z",
     "iopub.status.busy": "2025-09-17T22:56:10.862428Z",
     "iopub.status.idle": "2025-09-17T22:56:10.874028Z",
     "shell.execute_reply": "2025-09-17T22:56:10.873505Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "### Property: `current.huggingface_hub.loaded`\n",
       "\n",
       "Manages loaded HuggingFace models/datasets and provides access to their local paths.\n",
       "\n",
       "`current.huggingface_hub.loaded` provides a dictionary-like interface to access the local paths of the huggingface repos specified in the `load` argument of the `@huggingface_hub` decorator.\n",
       "\n",
       "#### Attributes\n",
       "\n",
       "- **info**: Returns metadata information about all loaded models from Hugging Face Hub.\n",
       "This property provides access to the metadata of models that have been loaded\n",
       "via the `@huggingface_hub(load=...)` decorator. The metadata includes information\n",
       "such as model repository details, storage location, and any cached information\n",
       "from the datastore. Returns a dictionary where keys are model repository IDs and values are metadata\n",
       "dictionaries containing information about each loaded model.\n",
       "\n",
       "#### Examples\n",
       "\n",
       "```python\n",
       "# Basic loading and access\n",
       "@huggingface_hub(load=[\"mistralai/Mistral-7B-Instruct-v0.1\"])\n",
       "@step\n",
       "def my_step(self):\n",
       "    # Access the local path of a loaded model\n",
       "    model_path = current.huggingface_hub.loaded[\"mistralai/Mistral-7B-Instruct-v0.1\"]\n",
       "\n",
       "    # Check if a model is loaded\n",
       "    if \"mistralai/Mistral-7B-Instruct-v0.1\" in current.huggingface_hub.loaded:\n",
       "        print(\"Model is loaded!\")\n",
       "\n",
       "# Custom path and advanced loading\n",
       "@huggingface_hub(load=[\n",
       "    (\"mistralai/Mistral-7B-Instruct-v0.1\", \"/custom/path\"),  # Specify custom path\n",
       "    {\n",
       "        \"repo_id\": \"org/model-name\",\n",
       "        \"force_download\": True,  # Force fresh download\n",
       "        \"repo_type\": \"dataset\"   # Load dataset instead of model\n",
       "    }\n",
       "])\n",
       "@step\n",
       "def another_step(self):\n",
       "    # Models are available at specified paths\n",
       "    pass\n",
       "```"
      ],
      "text/plain": [
       "<nbdoc.showdoc.ShowDoc at 0x7f616c476f20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowDoc(HuggingfaceLoadedModels, show_import=False, spoofstr=\"\", skip_sections=[\"Mf Add To Current\"], name=\"current.huggingface_hub.loaded\", hd_lvl=3, type_override=\"property\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T22:56:10.876247Z",
     "iopub.status.busy": "2025-09-17T22:56:10.875905Z",
     "iopub.status.idle": "2025-09-17T22:56:10.886168Z",
     "shell.execute_reply": "2025-09-17T22:56:10.885498Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/ob-packages-fresh-3/lib/python3.10/site-packages/numpydoc/docscrape.py:434: UserWarning: Unknown section Mf Add To Current in the docstring of ModelDecorator in /home/ubuntu/metaflow-checkpoint/metaflow_extensions/obcheckpoint/plugins/machine_learning_utilities/modeling_utils/decorator.py.\n",
      "  warn(msg)\n",
      "/home/ubuntu/miniconda3/envs/ob-packages-fresh-3/lib/python3.10/site-packages/numpydoc/docscrape.py:434: UserWarning: Unknown section @@ Returns in the docstring of ModelDecorator in /home/ubuntu/metaflow-checkpoint/metaflow_extensions/obcheckpoint/plugins/machine_learning_utilities/modeling_utils/decorator.py.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "# Decorator: `@model`\n",
       "\n",
       "```python\n",
       "@model(...)\n",
       "```\n",
       "\n",
       "**Module:** `metaflow`\n",
       "\n",
       "Enables loading / saving of models within a step.\n",
       "\n",
       "> Examples\n",
       "\n",
       "- Saving Models\n",
       "\n",
       "```python\n",
       "\n",
       "@model\n",
       "\n",
       "@step\n",
       "\n",
       "def train(self):\n",
       "\n",
       "    # current.model.save returns a dictionary reference to the model saved\n",
       "\n",
       "    self.my_model = current.model.save(\n",
       "\n",
       "        path_to_my_model,\n",
       "\n",
       "        label=\"my_model\",\n",
       "\n",
       "        metadata={\n",
       "\n",
       "            \"epochs\": 10,\n",
       "\n",
       "            \"batch-size\": 32,\n",
       "\n",
       "            \"learning-rate\": 0.001,\n",
       "\n",
       "        }\n",
       "\n",
       "    )\n",
       "\n",
       "    self.next(self.test)\n",
       "\n",
       "\n",
       "\n",
       "@model(load=\"my_model\")\n",
       "\n",
       "@step\n",
       "\n",
       "def test(self):\n",
       "\n",
       "    # `current.model.loaded` returns a dictionary of the loaded models\n",
       "\n",
       "    # where the key is the name of the artifact and the value is the path to the model\n",
       "\n",
       "    print(os.listdir(current.model.loaded[\"my_model\"]))\n",
       "\n",
       "    self.next(self.end)\n",
       "\n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "- Loading models\n",
       "\n",
       "```python\n",
       "\n",
       "@step\n",
       "\n",
       "def train(self):\n",
       "\n",
       "    # current.model.load returns the path to the model loaded\n",
       "\n",
       "    checkpoint_path = current.model.load(\n",
       "\n",
       "        self.checkpoint_key,\n",
       "\n",
       "    )\n",
       "\n",
       "    model_path = current.model.load(\n",
       "\n",
       "        self.model,\n",
       "\n",
       "    )\n",
       "\n",
       "    self.next(self.test)\n",
       "\n",
       "```\n",
       "\n",
       "## Parameters\n",
       "\n",
       "- **load** (*Union[List[str],str,List[Tuple[str,Union[str,None]]]], default: None*): Artifact name/s referencing the models/checkpoints to load. Artifact names refer to the names of the instance variables set to `self`.\n",
       "These artifact names give to `load` be reference objects or reference `key` string's from objects created by `current.checkpoint` / `current.model` / `current.huggingface_hub`.\n",
       "If a list of tuples is provided, the first element is the artifact name and the second element is the path the artifact needs be unpacked on\n",
       "the local filesystem. If the second element is None, the artifact will be unpacked in the current working directory.\n",
       "If a string is provided, then the artifact corresponding to that name will be loaded in the current working directory.\n",
       "- **temp_dir_root** (*str, default: None*): The root directory under which `current.model.loaded` will store loaded models"
      ],
      "text/plain": [
       "<nbdoc.showdoc.ShowDoc at 0x7f61541d9ba0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowDoc(model, show_import=True, decorator=True, skip_sections=[\"Mf Add To Current\"], markdown=True, hd_lvl=1)\n",
    "# You can use one of the following sections: \n",
    "# https://github.com/numpy/numpydoc/blob/865b865b3d1dfb8cd2139fa76cefe3643e1bf52a/numpydoc/docscrape.py#L118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T22:56:10.888306Z",
     "iopub.status.busy": "2025-09-17T22:56:10.887945Z",
     "iopub.status.idle": "2025-09-17T22:56:10.893339Z",
     "shell.execute_reply": "2025-09-17T22:56:10.892847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "## Method: `current.model.save`\n",
       "\n",
       "```python\n",
       "current.model.save(self, path, label=None, metadata=None, storage_format='tar')\n",
       "```\n",
       "\n",
       "Save a model to the datastore.\n",
       "\n",
       "### Parameters\n",
       "\n",
       "- **path** (*str or os.PathLike*): The path to the model file or directory to save. If a directory path is provided,\n",
       "all contents within that directory will be saved. If a file path is provided,\n",
       "the file will be directly saved to the datastore.\n",
       "- **label** (*str, optional*): A label to identify the saved model. If not provided, a default label based on\n",
       "the flow and step name will be used.\n",
       "- **metadata** (*dict, optional*): Additional metadata to store with the model. Default is None.\n",
       "- **storage_format** (*str, optional*): The storage format for the model. Must be one of STORAGE_FORMATS.TAR or\n",
       "STORAGE_FORMATS.FILES. Default is STORAGE_FORMATS.TAR.\n",
       "\n",
       "### Returns\n",
       "\n",
       "- **** (*dict*): A dictionary representation of the saved model artifact containing metadata\n",
       "and reference information.\n",
       "\n",
       "### Raises\n",
       "\n",
       "- **** (*ValueError*): If an unsupported storage format is provided."
      ],
      "text/plain": [
       "<nbdoc.showdoc.ShowDoc at 0x7f6154239a50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowDoc(ModelSerializer.save, show_import=False, skip_sections=[\"Mf Add To Current\"], name=\"current.model.save\", markdown=True, hd_lvl=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T22:56:10.896026Z",
     "iopub.status.busy": "2025-09-17T22:56:10.895682Z",
     "iopub.status.idle": "2025-09-17T22:56:10.900399Z",
     "shell.execute_reply": "2025-09-17T22:56:10.899817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "## Method: `current.model.load`\n",
       "\n",
       "```python\n",
       "current.model.load(self, reference: Union[str, metaflow_extensions.obcheckpoint.plugins.machine_learning_utilities.datastructures.MetaflowDataArtifactReference, dict], path: Optional[str] = None)\n",
       "```\n",
       "\n",
       "Load a model/checkpoint from the datastore to a temporary directory or a specified path.\n",
       "\n",
       "### Returns\n",
       "\n",
       "- **str** (*The path to the temporary directory where the model is loaded.*): "
      ],
      "text/plain": [
       "<nbdoc.showdoc.ShowDoc at 0x7f6154239a80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowDoc(ModelSerializer.load, show_import=False,  skip_sections=[\"Mf Add To Current\"], name=\"current.model.load\", markdown=True, hd_lvl=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T22:56:10.902718Z",
     "iopub.status.busy": "2025-09-17T22:56:10.902366Z",
     "iopub.status.idle": "2025-09-17T22:56:10.913441Z",
     "shell.execute_reply": "2025-09-17T22:56:10.912937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "## Property: `current.model.loaded`\n",
       "\n",
       "This property helps manage all the models loaded via `@model(load=...)` decorator and `current.model.load` method.\n",
       "\n",
       "It is a dictionary like object that stores the loaded models in a temporary directory. The keys of the dictionary are the artifact names and the values are the paths to the temporary directories where the models are stored.\n",
       "\n",
       "### Attributes\n",
       "\n",
       "- **info**: Returns metadata information about all loaded models.\n",
       "\n",
       "This property provides access to the metadata of models that have been loaded\n",
       "via the `@model(load=...)` decorator or `current.model.load` method. The metadata\n",
       "includes information such as model type, creation time, size, storage format,\n",
       "and any custom metadata that was saved with the model. For example setting\n",
       "`@model(load=[\"my_model\"])` will allow accessing it's metadata during flow runtime\n",
       "using `current.model.loaded.info[\"my_model\"]`\n",
       "\n",
       "### Examples\n",
       "\n",
       "```python\n",
       "    @model(load=[\"model_key\", \"chckpt_key\"])\n",
       "    @step\n",
       "    def mid_step(self):\n",
       "        import os\n",
       "        os.listdir(current.model.loaded[\"model_key\"])\n",
       "        os.listdir(current.model.loaded[\"chckpt_key\"])\n",
       "```"
      ],
      "text/plain": [
       "<nbdoc.showdoc.ShowDoc at 0x7f6154239c90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowDoc(LoadedModels, spoofstr=\"\", show_import=False, skip_sections=[\"Mf Add To Current\"], name=\"current.model.loaded\", markdown=True, hd_lvl=2, type_override=\"property\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T22:56:10.915714Z",
     "iopub.status.busy": "2025-09-17T22:56:10.915308Z",
     "iopub.status.idle": "2025-09-17T22:56:10.921585Z",
     "shell.execute_reply": "2025-09-17T22:56:10.921086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "## Function: `load_model`\n",
       "\n",
       "```python\n",
       "load_model(reference: Union[str, metaflow_extensions.obcheckpoint.plugins.machine_learning_utilities.datastructures.MetaflowDataArtifactReference, dict], path: str)\n",
       "```\n",
       "\n",
       "**Module:** `metaflow`\n",
       "\n",
       "Load a model or checkpoint from Metaflow's datastore to a local path.\n",
       "\n",
       "This function provides a convenient way to load models and checkpoints that were previously saved using `@model`, `@checkpoint`, or `@huggingface_hub` decorators, either from within a Metaflow task or externally using the Run API.\n",
       "\n",
       "### Parameters\n",
       "\n",
       "- **reference** (*Union[str, MetaflowDataArtifactReference, dict]*): The reference to the model/checkpoint to load. This can be A string key (e.g., \"model/my_model_abc123\") OR A MetaflowDataArtifactReference object OR a dictionary artifact reference (e.g., self.my_model from a previous step)\n",
       "- **path** (*str*): The local filesystem path where the model/checkpoint should be loaded. The directory will be created if it doesn't exist.\n",
       "\n",
       "### Raises\n",
       "\n",
       "- **** (*ValueError*): If reference or path is None\n",
       "- **** (*KeyNotCompatibleException*): If the reference key is not compatible with supported artifact types\n",
       "\n",
       "### Examples\n",
       "\n",
       "**Loading within a Metaflow task:**\n",
       "\n",
       "```python\n",
       "from metaflow import FlowSpec, step\n",
       "\n",
       "class MyFlow(FlowSpec):\n",
       "    @model\n",
       "    @step\n",
       "    def train(self):\n",
       "        # Save a model\n",
       "        self.my_model = current.model.save(\n",
       "            \"/path/to/trained/model\",\n",
       "            label=\"trained_model\"\n",
       "        )\n",
       "        self.next(self.evaluate)\n",
       "\n",
       "    @step\n",
       "    def evaluate(self):\n",
       "        from metaflow import load_model\n",
       "        # Load the model using the artifact reference\n",
       "        load_model(self.my_model, \"/tmp/loaded_model\")\n",
       "        # Model is now available at /tmp/loaded_model\n",
       "        self.next(self.end)\n",
       "```\n",
       "\n",
       "**Loading externally using Metaflow's Run API:**\n",
       "\n",
       "```python\n",
       "from metaflow import Run\n",
       "from metaflow import load_model\n",
       "\n",
       "# Get a reference to a completed run\n",
       "run = Run(\"MyFlow/123\")\n",
       "\n",
       "# Load using artifact reference from a step\n",
       "task_model_ref = run[\"train\"].task.data.my_model\n",
       "load_model(task_model_ref, \"/local/path/to/model\")\n",
       "\n",
       "model_ref = run.data.my_model\n",
       "load_model(model_ref, \"/local/path/to/model\")\n",
       "```\n",
       "\n",
       "**Loading HuggingFace models:**\n",
       "\n",
       "```python\n",
       "# If you saved a HuggingFace model reference\n",
       "@huggingface_hub\n",
       "@step\n",
       "def download_hf_model(self):\n",
       "    self.hf_model = current.huggingface_hub.snapshot_download(\n",
       "        repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\"\n",
       "    )\n",
       "    self.next(self.use_model)\n",
       "\n",
       "@step\n",
       "def use_model(self):\n",
       "    from metaflow import load_model\n",
       "    # Load the HuggingFace model\n",
       "    load_model(self.hf_model, \"/tmp/mistral_model\")\n",
       "    # Model files are now available at /tmp/mistral_model\n",
       "```"
      ],
      "text/plain": [
       "<nbdoc.showdoc.ShowDoc at 0x7f6154097e20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowDoc(load_model, show_import=True, decorator=False, skip_sections=[\"Mf Add To Current\"], markdown=True, hd_lvl=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T22:56:10.924240Z",
     "iopub.status.busy": "2025-09-17T22:56:10.923470Z",
     "iopub.status.idle": "2025-09-17T22:56:10.937212Z",
     "shell.execute_reply": "2025-09-17T22:56:10.936620Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/ob-packages-fresh-3/lib/python3.10/site-packages/numpydoc/docscrape.py:434: UserWarning: Unknown section Mf Add To Current in the docstring of CheckpointDecorator in /home/ubuntu/metaflow-checkpoint/metaflow_extensions/obcheckpoint/plugins/machine_learning_utilities/checkpoints/decorator.py.\n",
      "  warn(msg)\n",
      "/home/ubuntu/miniconda3/envs/ob-packages-fresh-3/lib/python3.10/site-packages/numpydoc/docscrape.py:434: UserWarning: Unknown section @@ Returns in the docstring of CheckpointDecorator in /home/ubuntu/metaflow-checkpoint/metaflow_extensions/obcheckpoint/plugins/machine_learning_utilities/checkpoints/decorator.py.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "# Decorator: `@checkpoint`\n",
       "\n",
       "```python\n",
       "@checkpoint(...)\n",
       "```\n",
       "\n",
       "**Module:** `metaflow`\n",
       "\n",
       "Enables checkpointing for a step.\n",
       "\n",
       "> Examples\n",
       "\n",
       "\n",
       "\n",
       "- Saving Checkpoints\n",
       "\n",
       "\n",
       "\n",
       "```python\n",
       "\n",
       "@checkpoint\n",
       "\n",
       "@step\n",
       "\n",
       "def train(self):\n",
       "\n",
       "    model = create_model(self.parameters, checkpoint_path = None)\n",
       "\n",
       "    for i in range(self.epochs):\n",
       "\n",
       "        # some training logic\n",
       "\n",
       "        loss = model.train(self.dataset)\n",
       "\n",
       "        if i % 10 == 0:\n",
       "\n",
       "            model.save(\n",
       "\n",
       "                current.checkpoint.directory,\n",
       "\n",
       "            )\n",
       "\n",
       "            # saves the contents of the `current.checkpoint.directory` as a checkpoint\n",
       "\n",
       "            # and returns a reference dictionary to the checkpoint saved in the datastore\n",
       "\n",
       "            self.latest_checkpoint = current.checkpoint.save(\n",
       "\n",
       "                name=\"epoch_checkpoint\",\n",
       "\n",
       "                metadata={\n",
       "\n",
       "                    \"epoch\": i,\n",
       "\n",
       "                    \"loss\": loss,\n",
       "\n",
       "                }\n",
       "\n",
       "            )\n",
       "\n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "- Using Loaded Checkpoints\n",
       "\n",
       "\n",
       "\n",
       "```python\n",
       "\n",
       "@retry(times=3)\n",
       "\n",
       "@checkpoint\n",
       "\n",
       "@step\n",
       "\n",
       "def train(self):\n",
       "\n",
       "    # Assume that the task has restarted and the previous attempt of the task\n",
       "\n",
       "    # saved a checkpoint\n",
       "\n",
       "    checkpoint_path = None\n",
       "\n",
       "    if current.checkpoint.is_loaded: # Check if a checkpoint is loaded\n",
       "\n",
       "        print(\"Loaded checkpoint from the previous attempt\")\n",
       "\n",
       "        checkpoint_path = current.checkpoint.directory\n",
       "\n",
       "\n",
       "\n",
       "    model = create_model(self.parameters, checkpoint_path = checkpoint_path)\n",
       "\n",
       "    for i in range(self.epochs):\n",
       "\n",
       "        ...\n",
       "\n",
       "```\n",
       "\n",
       "## Parameters\n",
       "\n",
       "- **load_policy** (*str, default: \"fresh\"*): The policy for loading the checkpoint. The following policies are supported:\n",
       "    - \"eager\": Loads the the latest available checkpoint within the namespace.\n",
       "    With this mode, the latest checkpoint written by any previous task (can be even a different run) of the step\n",
       "    will be loaded at the start of the task.\n",
       "    - \"none\": Do not load any checkpoint\n",
       "    - \"fresh\": Loads the lastest checkpoint created within the running Task.\n",
       "    This mode helps loading checkpoints across various retry attempts of the same task.\n",
       "    With this mode, no checkpoint will be loaded at the start of a task but any checkpoints\n",
       "    created within the task will be loaded when the task is retries execution on failure.\n",
       "- **temp_dir_root** (*str, default: None*): The root directory under which `current.checkpoint.directory` will be created."
      ],
      "text/plain": [
       "<nbdoc.showdoc.ShowDoc at 0x7f6154097eb0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowDoc(checkpoint, show_import=True, decorator=True, skip_sections=[\"Mf Add To Current\"], markdown=True, hd_lvl=1)\n",
    "# You can use one of the following sections: \n",
    "# https://github.com/numpy/numpydoc/blob/865b865b3d1dfb8cd2139fa76cefe3643e1bf52a/numpydoc/docscrape.py#L118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T22:56:10.940062Z",
     "iopub.status.busy": "2025-09-17T22:56:10.939651Z",
     "iopub.status.idle": "2025-09-17T22:56:10.943593Z",
     "shell.execute_reply": "2025-09-17T22:56:10.943015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "## Property: `current.checkpoint.directory`\n",
       "\n",
       "The directory where a checkpoint is loaded"
      ],
      "text/plain": [
       "<nbdoc.showdoc.ShowDoc at 0x7f6154238b80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowDoc(Checkpoint.directory, show_import=False, skip_sections=[\"Mf Add To Current\"], name=\"current.checkpoint.directory\",markdown=True, hd_lvl=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T22:56:10.945802Z",
     "iopub.status.busy": "2025-09-17T22:56:10.945473Z",
     "iopub.status.idle": "2025-09-17T22:56:10.957973Z",
     "shell.execute_reply": "2025-09-17T22:56:10.957477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "# Class: `Checkpoint`\n",
       "\n",
       "```python\n",
       "Checkpoint(temp_dir_root=None, init_dir=False)\n",
       "```\n",
       "\n",
       "**Module:** `metaflow`\n",
       "\n",
       "## Attributes\n",
       "\n",
       "- **directory**: The directory where a checkpoint is loaded"
      ],
      "text/plain": [
       "<nbdoc.showdoc.ShowDoc at 0x7f6154238a30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowDoc(Checkpoint, show_import=True, skip_sections=[\"Mf Add To Current\"], markdown=True, hd_lvl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T22:56:10.960221Z",
     "iopub.status.busy": "2025-09-17T22:56:10.959884Z",
     "iopub.status.idle": "2025-09-17T22:56:10.965112Z",
     "shell.execute_reply": "2025-09-17T22:56:10.964467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "## Method: `Checkpoint.save`\n",
       "\n",
       "```python\n",
       "Checkpoint.save(self, path=None, metadata=None, latest=True, name='mfchckpt', storage_format='files') -> Dict\n",
       "```\n",
       "\n",
       "Saves the checkpoint to the datastore\n",
       "\n",
       "### Parameters\n",
       "\n",
       "- **path** (*Optional[Union[str, os.PathLike]], default: None*): The path to save the checkpoint. Accepts a file path or a directory path.\n",
       "    - If a directory path is provided, all the contents within that directory will be saved.\n",
       "    When a checkpoint is reloaded during task retries, `the current.checkpoint.directory` will\n",
       "    contain the contents of this directory.\n",
       "    - If a file path is provided, the file will be directly saved to the datastore (with the same filename).\n",
       "    When the checkpoint is reloaded during task retries, the file with the same name will be available in the\n",
       "    `current.checkpoint.directory`.\n",
       "    - If no path is provided then the `Checkpoint.directory` will be saved as the checkpoint.\n",
       "- **name** (*Optional[str], default: \"mfchckpt\"*): The name of the checkpoint.\n",
       "- **metadata** (*Optional[Dict], default: {}*): Any metadata that needs to be saved with the checkpoint.\n",
       "- **latest** (*bool, default: True*): If True, the checkpoint will be marked as the latest checkpoint.\n",
       "This helps determine if the checkpoint gets loaded when the task restarts.\n",
       "- **storage_format** (*str, default: files*): If `tar`, the contents of the directory will be tarred before saving to the datastore.\n",
       "If `files`, saves directory directly to the datastore."
      ],
      "text/plain": [
       "<nbdoc.showdoc.ShowDoc at 0x7f616c476170>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowDoc(Checkpoint.save, show_import=False, skip_sections=[\"Mf Add To Current\"],markdown=True, hd_lvl=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T22:56:10.967365Z",
     "iopub.status.busy": "2025-09-17T22:56:10.967018Z",
     "iopub.status.idle": "2025-09-17T22:56:10.972844Z",
     "shell.execute_reply": "2025-09-17T22:56:10.971617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "## Method: `Checkpoint.load`\n",
       "\n",
       "```python\n",
       "Checkpoint.load(self, reference: Union[str, Dict, metaflow_extensions.obcheckpoint.plugins.machine_learning_utilities.datastructures.CheckpointArtifact], path: Optional[str] = None)\n",
       "```\n",
       "\n",
       "loads a checkpoint reference from the datastore. (resembles a read op)\n",
       "\n",
       "### Parameters\n",
       "\n",
       "- **`reference`**: - can be a string, dict or a CheckpointArtifact object:\n",
       "    - string: a string reference to the checkpoint (checkpoint key)\n",
       "    - dict: a dictionary reference to the checkpoint\n",
       "    - CheckpointArtifact: a CheckpointArtifact object reference to the checkpoint"
      ],
      "text/plain": [
       "<nbdoc.showdoc.ShowDoc at 0x7f616c475960>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowDoc(Checkpoint.load, show_import=False, skip_sections=[\"Mf Add To Current\"],markdown=True, hd_lvl=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T22:56:10.975058Z",
     "iopub.status.busy": "2025-09-17T22:56:10.974718Z",
     "iopub.status.idle": "2025-09-17T22:56:10.981293Z",
     "shell.execute_reply": "2025-09-17T22:56:10.980806Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "## Method: `Checkpoint.list`\n",
       "\n",
       "```python\n",
       "Checkpoint.list(self, name: Optional[str] = None, task: Union[ForwardRef('metaflow.Task'), str, NoneType] = None, attempt: Union[int, str, NoneType] = None, full_namespace: bool = False, as_dict: bool = True) -> List[Union[Dict, metaflow_extensions.obcheckpoint.plugins.machine_learning_utilities.datastructures.CheckpointArtifact]]\n",
       "```\n",
       "\n",
       "lists the checkpoints in the current task or the specified task.\n",
       "\n",
       "When users call `list` without any arguments, it will list all the checkpoints in the currently executing task (this includes all attempts). If the `list` method is called without any arguments outside a Metaflow Task execution context, it will raise an exception. Users can also call `list` with `attempt` argument to list all checkpoints within a the specific attempt of the currently executing task.\n",
       "\n",
       "\n",
       "\n",
       "When a `task` argument is provided, the `list` method will return all the checkpoints for a task's latest attempt unless a specific attempt number is set in the `attempt` argument. If the `Task` object contains a `DataArtifact` with all the previous checkpoints, then the `list` method will return all the checkpoints from the data artifact. If for some reason the DataArtifact is not written, then the `list` method will return all checkpoints directly from the checkpoint's datastore.\n",
       "\n",
       "### Parameters\n",
       "\n",
       "- **name** (*Optional[str], default: None*): Filter checkpoints by name.\n",
       "- **task** (*Optional[Union[\"metaflow.Task\", str]], default: None*): The task to list checkpoints from. Can be either a `Task` object or a task pathspec string.\n",
       "If None, lists checkpoints for the current task.\n",
       "Raises an exception if task is not provided when called outside a Metaflow Task execution context.\n",
       "- **attempt** (*Optional[Union[int, str]], default: None*): Filter checkpoints by attempt.\n",
       "If `task` is not None and `attempt` is None, then it will load the task's latest attempt\n",
       "- **full_namespace** (*bool, default: False*): If True, lists checkpoints from the full namespace.\n",
       "Only allowed during a Metaflow Task execution context.\n",
       "Raises an exception if `full_namespace` is set to True when called outside a Metaflow Task execution context.\n",
       "\n",
       "### Returns\n",
       "\n",
       "- **** (*List[Dict]*): \n",
       "\n",
       "### Examples\n",
       "\n",
       "```python\n",
       "\n",
       "Checkpoint().list(name=\"best\") # lists checkpoints in the current task with the name \"best\"\n",
       "Checkpoint().list(task=\"anotherflow/somerunid/somestep/sometask\", name=\"best\") # Identical as the above one but\n",
       "Checkpoint().list() # lists **all** the checkpoints in the current task (including the ones from all attempts)\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<nbdoc.showdoc.ShowDoc at 0x7f616c4777f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowDoc(Checkpoint.list, show_import=False, skip_sections=[\"Mf Add To Current\"],markdown=True, hd_lvl=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T20:58:01.035101Z",
     "iopub.status.busy": "2025-06-05T20:58:01.034821Z",
     "iopub.status.idle": "2025-06-05T20:58:01.037831Z",
     "shell.execute_reply": "2025-06-05T20:58:01.037481Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mf-modeling-utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
